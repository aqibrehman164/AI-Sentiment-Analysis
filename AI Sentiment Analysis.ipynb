{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8277266e-5a8e-4772-aec4-9ee996798e63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<br> \n",
    "<img src='https://s3.amazonaws.com/weclouddata/images/logos/wcd_logo_new_2.png' width='20%'>\n",
    "\n",
    "<h1 align='left'> Twitter Sentiment Analysis of AI topic in Databricks Cloud <font color='#559E54'> </font> </h1>\n",
    "<h3 align='left'> Databricks </h3>\n",
    "<h3 align='left'> By Malik Aqib Rehman </h3>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7012a8-c5c5-4425-b29f-e7a35bb62499",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounting weclouddata/twitter/\n",
      "/mnt/project has been unmounted.\n",
      "The bucket weclouddata/twitter/ was mounted to project \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mount_s3_bucket(access_key, secret_key, bucket_name, mount_folder):\n",
    "  ACCESS_KEY_ID = access_key\n",
    "  SECRET_ACCESS_KEY = secret_key\n",
    "  ENCODED_SECRET_KEY = SECRET_ACCESS_KEY.replace(\"/\", \"%2F\")\n",
    "\n",
    "  print (\"Mounting\", bucket_name)\n",
    "\n",
    "  try:\n",
    "    # Unmount the data in case it was already mounted.\n",
    "    dbutils.fs.unmount(\"/mnt/%s\" % mount_folder)\n",
    "    \n",
    "  except:\n",
    "    # If it fails to unmount it most likely wasn't mounted in the first place\n",
    "    print (\"Directory not unmounted: \", mount_folder)\n",
    "    \n",
    "  finally:\n",
    "    # Lastly, mount our bucket.\n",
    "    dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY_ID, ENCODED_SECRET_KEY, bucket_name), \"/mnt/%s\" % mount_folder)\n",
    "    # dbutils.fs.mount(\"s3a://\"+ ACCESS_KEY_ID + \":\" + ENCODED_SECRET_KEY + \"@\" + bucket_name, mount_folder)\n",
    "    print (\"The bucket\", bucket_name, \"was mounted to\", mount_folder, \"\\n\")\n",
    "\n",
    "# Set AWS programmatic access credentials\n",
    "ACCESS_KEY = \"AKIA3QEBZIOD6R6BV6NA\"\n",
    "SECRET_ACCESS_KEY = \"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\"\n",
    "\n",
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, \"weclouddata/twitter/\", \"project\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e11167-0d93-4535-801c-72d5e0a09bb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-24-27-2be53775-fbca-36fb-8bab-a9e84803a793</td><td>topic4-2-2022-12-08-18-24-27-2be53775-fbca-36fb-8bab-a9e84803a793</td><td>7676</td><td>1670524170000</td></tr><tr><td>dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-29-11-6b698cf3-971d-3c2f-a0c0-ae88d5c8b9cd</td><td>topic4-2-2022-12-08-18-29-11-6b698cf3-971d-3c2f-a0c0-ae88d5c8b9cd</td><td>16486</td><td>1670524454000</td></tr><tr><td>dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-34-07-612ecb35-016e-3cac-8d5d-c89e4129774c</td><td>topic4-2-2022-12-08-18-34-07-612ecb35-016e-3cac-8d5d-c89e4129774c</td><td>12526</td><td>1670524750000</td></tr><tr><td>dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-38-58-93f597ec-f4af-3ba8-9050-ac4db971fd62</td><td>topic4-2-2022-12-08-18-38-58-93f597ec-f4af-3ba8-9050-ac4db971fd62</td><td>8227</td><td>1670525040000</td></tr><tr><td>dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-43-42-8264e752-504a-3e57-b227-055bedd8b93a</td><td>topic4-2-2022-12-08-18-43-42-8264e752-504a-3e57-b227-055bedd8b93a</td><td>11783</td><td>1670525324000</td></tr><tr><td>dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-48-49-c40fe1fa-ace1-30cf-8c3d-56376756e989</td><td>topic4-2-2022-12-08-18-48-49-c40fe1fa-ace1-30cf-8c3d-56376756e989</td><td>9315</td><td>1670525632000</td></tr><tr><td>dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-53-38-62d1eb45-6705-3cb3-8b91-f5bcff482f8d</td><td>topic4-2-2022-12-08-18-53-38-62d1eb45-6705-3cb3-8b91-f5bcff482f8d</td><td>8345</td><td>1670525921000</td></tr><tr><td>dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-58-35-25aee653-cb0c-3580-aab1-02af2820cd6b</td><td>topic4-2-2022-12-08-18-58-35-25aee653-cb0c-3580-aab1-02af2820cd6b</td><td>8446</td><td>1670526219000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-24-27-2be53775-fbca-36fb-8bab-a9e84803a793",
         "topic4-2-2022-12-08-18-24-27-2be53775-fbca-36fb-8bab-a9e84803a793",
         7676,
         1670524170000
        ],
        [
         "dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-29-11-6b698cf3-971d-3c2f-a0c0-ae88d5c8b9cd",
         "topic4-2-2022-12-08-18-29-11-6b698cf3-971d-3c2f-a0c0-ae88d5c8b9cd",
         16486,
         1670524454000
        ],
        [
         "dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-34-07-612ecb35-016e-3cac-8d5d-c89e4129774c",
         "topic4-2-2022-12-08-18-34-07-612ecb35-016e-3cac-8d5d-c89e4129774c",
         12526,
         1670524750000
        ],
        [
         "dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-38-58-93f597ec-f4af-3ba8-9050-ac4db971fd62",
         "topic4-2-2022-12-08-18-38-58-93f597ec-f4af-3ba8-9050-ac4db971fd62",
         8227,
         1670525040000
        ],
        [
         "dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-43-42-8264e752-504a-3e57-b227-055bedd8b93a",
         "topic4-2-2022-12-08-18-43-42-8264e752-504a-3e57-b227-055bedd8b93a",
         11783,
         1670525324000
        ],
        [
         "dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-48-49-c40fe1fa-ace1-30cf-8c3d-56376756e989",
         "topic4-2-2022-12-08-18-48-49-c40fe1fa-ace1-30cf-8c3d-56376756e989",
         9315,
         1670525632000
        ],
        [
         "dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-53-38-62d1eb45-6705-3cb3-8b91-f5bcff482f8d",
         "topic4-2-2022-12-08-18-53-38-62d1eb45-6705-3cb3-8b91-f5bcff482f8d",
         8345,
         1670525921000
        ],
        [
         "dbfs:/mnt/project/AI/2022/12/08/18/topic4-2-2022-12-08-18-58-35-25aee653-cb0c-3580-aab1-02af2820cd6b",
         "topic4-2-2022-12-08-18-58-35-25aee653-cb0c-3580-aab1-02af2820cd6b",
         8446,
         1670526219000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /mnt/project/AI/2022/12/08/18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ba8e52-c64f-49e1-9534-9a51bacf98f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "# pip install vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c261ee2f-2c72-46c2-b79b-1ef921826c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Twitter Project') \\\n",
    "        .getOrCreate()\n",
    "print('Session created')\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0338b0-a69e-48c9-8bc4-66c3a0beb7a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "\n",
    "# Define the schema\n",
    "twitterSchema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True),\n",
    "    StructField(\"followers_count\", LongType(), True),  # Assuming public_metrics includes followers_count \n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"n1\", StringType(), True),\n",
    "    StructField(\"created_at\", StringType(), True)\n",
    "    # StructField(\"n2\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce31919-d8b2-48c9-87d7-c6513e906b99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tweets = spark.read \\\n",
    ".option('delimiter', '\\t') \\\n",
    ".option('header', 'true') \\\n",
    ".schema(twitterSchema) \\\n",
    ".csv('/mnt/project/AI/*/*/*/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1dab63-6557-4452-9ed5-442bd6975c5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c4002b-e6ff-44a4-85ee-7684fa79e5d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- followers_count: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- n1: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cache the dataframe for faster iteration\n",
    "tweets.cache() \n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b21e8f-f0cf-42c5-af5a-325239baf299",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: 10208"
     ]
    }
   ],
   "source": [
    "# run the count action to materialize the cache\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eaa572f-ada8-4ba1-8c35-cff193c3bcd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'tweets' that you want to modify\n",
    "tweets = tweets.drop(\"n1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38573c3-d98b-4f14-9794-c929c0b15e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9095190043726875\n",
      "F1 Score: 0.9097816913197355\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer,\n",
    "    StopWordsRemover,\n",
    "    HashingTF,\n",
    "    StringIndexer,\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "tweets_clean = tweets.withColumn(\"tweet\", F.regexp_replace(\"tweet\", r\"http\\S+\", \"\")) \\\n",
    "    .withColumn(\"tweet\", F.regexp_replace(\"tweet\", r\"[^a-zA-Z]\", \" \")) \\\n",
    "    .withColumn(\"tweet\", F.regexp_replace(\"tweet\", r\"\\s+\", \" \")) \\\n",
    "    .withColumn(\"tweet\", F.lower(\"tweet\")) \\\n",
    "    .withColumn(\"tweet\", F.trim(\"tweet\"))\n",
    "\n",
    "# Define a UDF to get sentiment labels\n",
    "def get_sentiment(text):\n",
    "    if text is not None:  # Check for null values\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        \n",
    "        # Assign a sentiment label based on the compound score\n",
    "        if sentiment['compound'] >= 0.05:\n",
    "            return \"positive\"\n",
    "        elif sentiment['compound'] <= -0.05:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    else:\n",
    "        return \"unknown\"  # Handle null values with an appropriate label\n",
    "\n",
    "# Create a User-Defined Function (UDF) for sentiment analysis\n",
    "sentiment_udf = udf(get_sentiment, StringType())\n",
    "\n",
    "# Assuming 'tweets_clean' is your DataFrame with a 'tweet' column\n",
    "# Filter out rows with null values in the 'tweet' column\n",
    "tweets_clean = tweets_clean.filter(tweets_clean[\"tweet\"].isNotNull())\n",
    "\n",
    "# Create a new column 'sentiment' with sentiment labels\n",
    "tweets_clean = tweets_clean.withColumn(\"sentiment\", sentiment_udf(tweets_clean[\"tweet\"]))\n",
    "\n",
    "# Preprocessing and Feature Engineering\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"features\")\n",
    "label_indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n",
    "\n",
    "# Create and Configure the Machine Learning Model (Logistic Regression)\n",
    "logistic_regression = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=100)\n",
    "\n",
    "# Create the ML Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, hashing_tf, label_indexer, logistic_regression])\n",
    "\n",
    "# Split the Data into Training and Testing Sets\n",
    "(training_data, testing_data) = tweets_clean.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "# Train the Model\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Make Predictions on the Testing Data\n",
    "predictions = model.transform(testing_data)\n",
    "\n",
    "# Evaluate the Model for Accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Display Accuracy\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create a MulticlassClassificationEvaluator for F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Calculate F1-score\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "384eef9c-5f7f-4e18-8406-f38845c18dd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounting b18-malik\n",
      "/mnt/my_bucket has been unmounted.\n",
      "The bucket b18-malik was mounted to my_bucket \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Save to Your S3 Bucket\n",
    "# # Mount your own bucket\n",
    "mount_s3_bucket(\n",
    "    ACCESS_KEY,\n",
    "    SECRET_ACCESS_KEY,\n",
    "    \"b18-malik\",\n",
    "    \"my_bucket\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61408344-3269-444c-b18d-718c20271ff5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Save the cleaned 'tweets' DataFrame to S3\n",
    "tweets_clean.write.option('header', 'false').option('delimiter', '\\t').csv(\"/mnt/my_bucket/raw_tweets_11/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f67e7ca2-0b0a-440e-aead-dc18b0cc552f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Prepare the predictions DataFrame\n",
    "\n",
    "# Remove vector or array type columns from the predictions DataFrame\n",
    "columns_to_drop = [\"features\", \"rawPrediction\", \"probability\", \"words\", \"filtered_words\" ]  # List of columns to drop\n",
    "\n",
    "# Select relevant columns for saving to S3\n",
    "predictions = predictions.select([col for col in predictions.columns if col not in columns_to_drop])\n",
    "\n",
    "# Save the predictions DataFrame to S3\n",
    "predictions.write.option('header', 'false').option('delimiter', '\\t').csv(\"/mnt/my_bucket/predictions_10/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b8f5092-c11a-4a2d-8b2b-01a115cb7d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 163924673591436,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Malik Aqib Final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
